<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="3D Foundation Policy for Robotic Manipulation.">
  <meta name="keywords" content="FP3, 3D Foundation Policy, Robotic Manipulation, Robotics, Machine Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FP3: A 3D Foundation Policy for Robotic Manipulation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/thu_logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <style>
    #videoGrid {
      width: 100%;
      margin: 0 auto;
      display: grid;
      grid-template-columns: repeat(3, 1fr);
    }
    #videoGrid video {
      width: 100%;
      height: auto;
    }
  </style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">FP3: A 3D Foundation Policy for Robotic Manipulation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a>Rujia Yang</a><sup>*1</sup>,</span>
            <span class="author-block">
              <a>Geng Chen</a><sup>*2,4</sup>,</span>
            <span class="author-block">
              <a>Chuan Wen</a><sup>‡1,2,3</sup>,
            </span>
            <span class="author-block">
              <a>Yang Gao</a><sup>‡1,2,3</sup>,
            </span>
            <!-- <span class="author-block">
              <a href="http://sofienbouaziz.com">Sofien Bouaziz</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.danbgoldman.com">Dan B Goldman</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Ricardo Martin-Brualla</a><sup>2</sup>
            </span> -->
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>IIIS, Tsinghua University,</span>
            <span class="author-block"><sup>2</sup>Shanghai AI Laboratory,</span>
            <span class="author-block"><sup>3</sup>Shanghai Qi Zhi Institute,</span>
            <span class="author-block"><sup>4</sup>UC San Diego</span>
            <span class="author-block"><sup>*</sup>Equal Contribution</span>
            <span class="author-block"><sup>‡</sup>Equal Advising</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2503.08950"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. (Coming Soon)-->
              <span class="link-block">
                <a href="https://github.com/horipse01/3d-foundation-policy"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <div class="publication-video">
        <iframe src="./static/images/fp3_video.mp4"
                frameborder="0" allowfullscreen width="80%" height="180"></iframe>
      </div>

    </div>
  </div>
  <div style="text-align: center;">
    <img src="./static/concept.jpg" width="80%" height="600px", alt="FP3 Concept">
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Following its success in natural language processing and computer vision, foundation models that are pre-trained on large-scale multi-task datasets have also shown great potential in robotics. However, most existing robot foundation models rely solely on 2D image observations, ignoring 3D geometric information, which is essential for robots to perceive and reason about the 3D world. In this paper, we introduce FP3, a first large-scale 3D foundation policy model for robotic manipulation. FP3 builds on a scalable diffusion transformer architecture and is pre-trained on 60k trajectories with point cloud observations. With the model design and diverse pre-training data, FP3 can be efficiently fine-tuned for downstream tasks while exhibiting strong generalization capabilities. Experiments on real robots demonstrate that with only 80 demonstrations, FP3 is able to learn a new task with over 90% success rates in novel environments with unseen objects, significantly surpassing existing robot foundation models.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">In-the-Wild Test</h2>
        <div class="content has-text-justified">
          <p>
            Our method, FP3, can be fine-tuned with a small number of target task samples and directly generalize to unseen scenes and objects. Our method outperforms both small models without pretraining and other large robotic models.
          </p>
        </div>
        <div class="select" style="display: flex; justify-content: center; align-items: center; height: 10vh;">
          <select id="video-selection" style="width: 100%;">
              <option value="group1">Fold Towel</option>
              <option value="group2">Stand Up Cup</option>
              <option value="group3">Clean Table</option>
              <option value="group4">Pour Water</option>
          </select>
        </div>
        <div id="video-container" style="display: flex; flex-direction: column; justify-content: center; align-items: center; width: 100%;">
          <video controls id="video1" src="./static/videos/video1.mp4" style="width: 100%;"></video>
          <video controls id="video2" src="./static/videos/video8.mp4" style="width: 100%;"></video>
          <video controls id="video3" src="./static/videos/video15.mp4" style="width: 100%;"></video>
          <video controls id="video4" src="./static/videos/video16.mp4" style="width: 100%;"></video>
      </div>

        <script>
            const videoSets = {
                group1: ["./static/videos/video1.mp4", "./static/videos/video8.mp4", "./static/videos/video15.mp4", "./static/videos/video16.mp4"],
                group2: ["./static/videos/video2.mp4", "./static/videos/video11.mp4", "./static/videos/video13.mp4", "./static/videos/video17.mp4"],
                group3: ["./static/videos/video18.mp4", "./static/videos/video4.mp4", "./static/videos/video6.mp4", "./static/videos/video19.mp4"],
                group4: ["./static/videos/video10.mp4", "./static/videos/video20.mp4", "./static/videos/video14.mp4", "./static/videos/video21.mp4"]
            };

            document.getElementById("video-selection").addEventListener("change", function() {
                const selectedGroup = this.value;
                const videos = document.querySelectorAll("#video-container video");
                videoSets[selectedGroup].forEach((src, index) => {
                    videos[index].src = src;
                });
            });
        </script>
      </div>
    </div>
  </div>
</section>

<section class="section">
      <!-- Visual Effects. -->
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Generalization</h2>
        <div class="content has-text-justified">
          <p>
            We conduct more comprehensive experiments on FP3's generalizability to different environments and robot setups using the Clean Table task.
          </p>
        </div>
        <div class="select" style="display: flex; justify-content: center; align-items: center; height: 10vh;">
          <select id="task-selection" style="width: 100%;">
            <option value="./static/videos/in_domain_converted.mp4">In Domain</option>
            <option value="./static/videos/background_converted.mp4">Unseen Background</option>
            <option value="./static/videos/distractor_converted.mp4">Unseen Distractor</option>
            <option value="./static/videos/object_converted.mp4">Unseen Object</option>
            <option value="./static/videos/light_converted.mp4">Unseen Light Condition</option>
            <option value="./static/videos/camera_converted.mp4">Unseen Camera View</option>
          </select>
        </div>          
        <video id="generalization" autoplay controls muted loop playsinline width="100%">
            <source id="video-source" src="./static/videos/in_domain_converted.mp4" type="video/mp4">
        </video>
        <script>
            document.getElementById('task-selection').addEventListener('change', function() {
                let videoPlayer = document.getElementById('generalization');
                let videoSource = document.getElementById('video-source');
                let selectedVideo = this.value;

                videoSource.src = selectedVideo; // 修改 source 的 src
                videoPlayer.load(); // 重新加载视频
                videoPlayer.play(); // 播放新的视频
            });
        </script>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Instruction Following Performance</h2>
        <div class="content has-text-justified">
          <p>
            To demonstrate FP3's language instruction following capability, we trained a multi-task policy using data from all four tasks and designed experiments to verify that even in the same scene setup, FP3 can still correctly execute different tasks when given different language instructions.
          </p>
        </div>
      </div>
    </div>

    <!-- Video Grid. -->
    <div class="container" id="videoGrid" style="width: 100%; margin: 0 auto; display: grid; grid-template-columns: repeat(2, 1fr); gap: 10px;">
      <div>
          <h3>Clean Table</h3>
          <video src="./static/videos/clean.mp4" autoplay loop muted style="width: 100%;"></video>
      </div>

      <!-- 视频 2 -->
      <div>
          <h3>Stand Up Cup</h3>
          <video src="./static/videos/stand.mp4" autoplay loop muted style="width: 100%;"></video>
      </div>

      <!-- 视频 3 -->
      <div>
          <h3>Fold Towel</h3>
          <video src="./static/videos/fold.mp4" autoplay loop muted style="width: 100%;"></video>
      </div>

      <!-- 视频 4 -->
      <div>
          <h3>Pour Water</h3>
          <video src="./static/videos/pour.mp4" autoplay loop muted style="width: 100%;"></video>
      </div>
    </div>
  </div>
</section>
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{yang2025fp33dfoundationpolicy,
      title={FP3: A 3D Foundation Policy for Robotic Manipulation}, 
      author={Rujia Yang and Geng Chen and Chuan Wen and Yang Gao},
      year={2025},
      eprint={2503.08950},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2503.08950}, 
}</code></pre>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop content has-text-centered">
    <p>The website template is sourced from <a href="https://github.com/nerfies/nerfies.github.io">https://github.com/nerfies/nerfies.github.io</a></p>
  </div>
</section>
</body>
</html>
